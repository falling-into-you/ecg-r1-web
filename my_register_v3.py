import os

import sys
import types
import re
import random
import copy
from typing import Any, Dict, List, Literal, Optional, Tuple, Union

import torch
import torch.nn as nn
import numpy as np

# ==================== 1. åŸºç¡€ä¾èµ– ====================
from transformers import AutoConfig, AutoModel, AutoTokenizer
# æ˜ç¡®å¼•å…¥ Qwen3VL ç›¸å…³ç±»
from transformers import Qwen3VLForConditionalGeneration
from transformers.models.qwen3_vl.modeling_qwen3_vl import Qwen3VLModelOutputWithPast

from swift.llm import (
    Model, ModelMeta, MultiModelKeys, Template, TemplateMeta,
    get_model_tokenizer, register_model, register_model_arch, register_template,
    get_template
)
# ç›´æ¥å¼•å…¥ Qwen3VLTemplate
from swift.llm.template.template.qwen import Qwen3VLTemplate, QwenTemplateMeta
from swift.llm.template.template_inputs import StdTemplateInputs
from swift.llm.template.utils import Context, findall
from swift.utils import get_env_args, get_logger
logger = get_logger()

# åŠ¨æ€è®¡ç®—é¡¹ç›®è·¯å¾„
_CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(_CURRENT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)


# ==================== 2. ECG ç»„ä»¶æ„å»ºå·¥å…· ====================

def build_ecg_tower(ecg_tower_path: str, model_config_name: str = 'coca_ViT-B-32', device: str = 'cpu'):
    """æ„å»º ECG Tower"""
    from ecg_coca.training import get_ecg_encoder
    ecg_tower, ecg_processor, ecg_config = get_ecg_encoder(
        model_name=model_config_name,
        checkpoint_path=ecg_tower_path,
        device=device
    )
    logger.info(f'Loaded ECG tower from {ecg_tower_path}')
    return ecg_tower, ecg_config

def build_ecg_projector(ecg_hidden_size: int, llm_hidden_size: int, projector_type: str = 'mlp2x_gelu'):
    """æ„å»º Projector"""
    if projector_type == 'linear':
        return nn.Linear(ecg_hidden_size, llm_hidden_size)
    
    match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
    if match:
        mlp_depth = int(match.group(1))
        modules = [nn.Linear(ecg_hidden_size, llm_hidden_size)]
        for _ in range(1, mlp_depth):
            modules.append(nn.GELU())
            modules.append(nn.Linear(llm_hidden_size, llm_hidden_size))
        return nn.Sequential(*modules)
    
    raise ValueError(f'Unknown projector type: {projector_type}')

def load_ecg(ecg_path: str, ecg_seq_length: Optional[int] = 5000, root_ecg_dir: Optional[str] = None) -> torch.Tensor:
    """åŠ è½½ ECG æ•°æ® (WFDB)"""
    import wfdb
    if isinstance(ecg_path, torch.Tensor):
        return ecg_path
    
    path = ecg_path
    if root_ecg_dir and not os.path.isabs(path):
        path = os.path.join(root_ecg_dir, path)
    
    try:
        ecg_data = wfdb.rdsamp(path)[0]
    except Exception as e:
        raise ValueError(f"Failed to load ECG from {path}: {e}")
    
    ecg_data[np.isnan(ecg_data)] = 0
    ecg_data[np.isinf(ecg_data)] = 0
    # (L, C) -> (C, L)
    ecg_tensor = torch.from_numpy(np.transpose(ecg_data, (1, 0)).astype(np.float32))
    
    c, length = ecg_tensor.shape
    if ecg_seq_length is not None:
        if length < ecg_seq_length:
            new_tensor = torch.zeros((c, ecg_seq_length), dtype=ecg_tensor.dtype)
            new_tensor[:, 0:length] = ecg_tensor
            ecg_tensor = new_tensor
        elif length > ecg_seq_length:
            ecg_tensor = ecg_tensor[:, 0:ecg_seq_length]
    return ecg_tensor


# ==================== 3. è‡ªå®šä¹‰ Backbone Forward é€»è¾‘ ====================

def qwen3vl_backbone_forward_with_ecg(
    self, # self æŒ‡å‘ model.model (Backbone)
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[Any] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    pixel_values: Optional[torch.Tensor] = None,
    pixel_values_videos: Optional[torch.FloatTensor] = None,
    image_grid_thw: Optional[torch.LongTensor] = None,
    video_grid_thw: Optional[torch.LongTensor] = None,
    cache_position: Optional[torch.LongTensor] = None,
    # æ–°å¢å‚æ•°
    ecg_features: Optional[torch.FloatTensor] = None,
    **kwargs,
):
    """
    ç»‘å®šåˆ° Backbone ä¸Šçš„ Forward æ–¹æ³•ã€‚
    """
    from transformers.utils import is_torchdynamo_compiling
    
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

    if inputs_embeds is None:
        inputs_embeds = self.get_input_embeddings()(input_ids)
        
        if input_ids is not None:
            input_ids = input_ids.to(inputs_embeds.device)

    # --- 1. åŸç”Ÿè§†è§‰ (Image) ---
    image_mask = None
    if pixel_values is not None:
        image_embeds, deepstack_image_embeds = self.get_image_features(pixel_values, image_grid_thw)
        image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        image_mask, _ = self.get_placeholder_mask(
            input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds
        )
        inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)

    # --- 2. åŸç”Ÿè§†è§‰ (Video) ---
    video_mask = None
    if pixel_values_videos is not None:
        video_embeds, deepstack_video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)
        video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        _, video_mask = self.get_placeholder_mask(
            input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds
        )
        inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)

    # --- 3. ECG å¤„ç† (æ–°å¢) ---
    if ecg_features is not None:
        if hasattr(self, 'ecg_tower') and hasattr(self, 'ecg_projector'):
            # ç¡®ä¿åœ¨åŒä¸€è®¾å¤‡
            ecg_tower_device = next(self.ecg_tower.parameters()).device
            ecg_features = ecg_features.to(ecg_tower_device, inputs_embeds.dtype)
            
            if not ecg_features.requires_grad:
                 ecg_features.requires_grad_(True)
            
            # Forward: Tower -> Projector
            ecg_embeds = self.ecg_tower(ecg_features, output_last_transformer_layer=True)
            ecg_embeds = self.ecg_projector(ecg_embeds)
            
            # ç§»å› embedding è®¾å¤‡
            ecg_embeds = ecg_embeds.to(inputs_embeds.device, inputs_embeds.dtype)
            
            # èåˆé€»è¾‘
            ecg_token_id = getattr(self.config, 'ecg_token_id', None)
            if ecg_token_id is not None and input_ids is not None:
                ecg_mask = (input_ids == ecg_token_id)
                n_ecg_tokens = ecg_mask.sum()
                if n_ecg_tokens > 0:
                    ecg_embeds_flat = ecg_embeds.reshape(-1, ecg_embeds.shape[-1])
                    if ecg_embeds_flat.shape[0] >= n_ecg_tokens:
                        ecg_embeds_flat = ecg_embeds_flat[:n_ecg_tokens]
                        inputs_embeds[ecg_mask] = ecg_embeds_flat.to(inputs_embeds.dtype)
        else:
            logger.warning_once("ECG features provided but model has no ecg_tower attached.")

    # --- 4. DeepStack å‡†å¤‡ (åŸç”Ÿ Qwen3VL é€»è¾‘) ---
    visual_pos_masks = None
    deepstack_visual_embeds = None
    if image_mask is not None and video_mask is not None:
        image_mask = image_mask[..., 0]
        video_mask = video_mask[..., 0]
        visual_pos_masks = image_mask | video_mask
        deepstack_visual_embeds = []
        image_mask_joint = image_mask[visual_pos_masks]
        video_mask_joint = video_mask[visual_pos_masks]
        for img_embed, vid_embed in zip(deepstack_image_embeds, deepstack_video_embeds):
            embed_joint = img_embed.new_zeros(visual_pos_masks.sum(), img_embed.shape[-1]).to(img_embed.device)
            embed_joint[image_mask_joint, :] = img_embed
            embed_joint[video_mask_joint, :] = vid_embed
            deepstack_visual_embeds.append(embed_joint)
    elif image_mask is not None:
        image_mask = image_mask[..., 0]
        visual_pos_masks = image_mask
        deepstack_visual_embeds = deepstack_image_embeds
    elif video_mask is not None:
        video_mask = video_mask[..., 0]
        visual_pos_masks = video_mask
        deepstack_visual_embeds = deepstack_video_embeds

    # --- 5. RoPE å‡†å¤‡ (åŸç”Ÿ Qwen3VL é€»è¾‘) ---
    if position_ids is None:
        attention_mask_tensor = (
            attention_mask if not isinstance(attention_mask, dict) else attention_mask["full_attention"]
        )
        if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:
            attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)
            if attention_mask_tensor.dtype.is_floating_point:
                attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min
                attention_mask_tensor = (1.0 - attention_mask_tensor).int()

        prefill_compiled_stage = is_torchdynamo_compiling() and (
            (input_ids is not None and input_ids.shape[1] != 1)
            or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)
        )
        prefill_noncompiled_stage = not is_torchdynamo_compiling() and (
            (cache_position is not None and cache_position[0] == 0)
            or (past_key_values is None or past_key_values.get_seq_length() == 0)
        )
        if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:
            position_ids, rope_deltas = self.get_rope_index(
                input_ids, image_grid_thw, video_grid_thw, attention_mask=attention_mask_tensor,
            )
            self.rope_deltas = rope_deltas
        else:
            batch_size, seq_length, _ = inputs_embeds.shape
            delta = (
                (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)
                if cache_position is not None
                else 0
            )
            position_ids = torch.arange(seq_length, device=inputs_embeds.device)
            position_ids = position_ids.view(1, -1).expand(batch_size, -1)
            if cache_position is not None:
                delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=0)
            position_ids = position_ids.add(delta)
            position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)

    # --- 6. è°ƒç”¨ LLM ---
    outputs = self.language_model(
        input_ids=None,
        position_ids=position_ids,
        attention_mask=attention_mask,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        cache_position=cache_position,
        visual_pos_masks=visual_pos_masks,
        deepstack_visual_embeds=deepstack_visual_embeds,
        **kwargs,
    )

    # --- 7. è¿”å› Qwen3VL è¾“å‡ºå¯¹è±¡ ---
    return Qwen3VLModelOutputWithPast(
        last_hidden_state=outputs.last_hidden_state,
        past_key_values=outputs.past_key_values,
        hidden_states=outputs.hidden_states,
        attentions=outputs.attentions,
        rope_deltas=self.rope_deltas,
    )


# ==================== 4. ECGR1ForConditionalGeneration ç±» ====================

class ECGR1ForConditionalGeneration(Qwen3VLForConditionalGeneration):
    """
    ECG-R1 æ¨¡å‹ç±»ï¼Œç»§æ‰¿è‡ª Qwen3VLForConditionalGenerationã€‚
    å†…éƒ¨è‡ªåŠ¨æŒ‚è½½ ECG ç»„ä»¶ï¼Œå¹¶åŠ«æŒ backbone çš„ forward é€»è¾‘ã€‚
    """
    def __init__(self, config):
        super().__init__(config)
        
        # 1. åˆå§‹åŒ– ECG ç»„ä»¶
        self._init_ecg_components(config)
        
        # 2. ç»‘å®šè‡ªå®šä¹‰ forward åˆ° backbone (self.model)
        if hasattr(self, 'model'):
            self.model.forward = types.MethodType(qwen3vl_backbone_forward_with_ecg, self.model)
            logger.info('âœ… ECGR1: Bound custom forward method to backbone model.')
        else:
            logger.error('âŒ ECGR1: self.model not found, initialization failed.')

    def _init_ecg_components(self, config):
        # Prioritize environment variable to allow overriding config (which might contain relative paths)
        ecg_tower_path = get_env_args('ECG_TOWER_PATH', str, None) or getattr(config, 'ecg_tower_path', None)
        ecg_projector_type = get_env_args('ECG_PROJECTOR_TYPE', str, None) or getattr(config, 'ecg_projector_type', 'mlp2x_gelu')
        ecg_model_config = get_env_args('ECG_MODEL_CONFIG', str, None) or getattr(config, 'ecg_model_config', 'coca_ViT-B-32')

        llm_hidden_size = getattr(config, 'hidden_size', None)
        if llm_hidden_size is None and hasattr(config, 'text_config'):
             llm_hidden_size = getattr(config.text_config, 'hidden_size', None)
        
        if ecg_tower_path and llm_hidden_size:
            # é¿å…é‡å¤åŠ è½½
            if hasattr(self.model, 'ecg_tower'):
                return

            logger.info(f'Initializing ECG components from {ecg_tower_path}...')
            try:
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                ecg_tower, ecg_cfg = build_ecg_tower(ecg_tower_path, ecg_model_config, device=device)
                ecg_hidden_size = ecg_cfg.get('ecg_cfg', {}).get('width', 768)
                ecg_projector = build_ecg_projector(ecg_hidden_size, llm_hidden_size, ecg_projector_type)
                
                # Handle meta device initialization (accelerate)
                if any(p.device.type == 'meta' for p in ecg_projector.parameters()):
                    ecg_projector.to_empty(device=device)
                    # Re-initialize parameters since to_empty() leaves them uninitialized
                    def _init_weights(m):
                        if isinstance(m, nn.Linear):
                            nn.init.xavier_uniform_(m.weight)
                            if m.bias is not None:
                                nn.init.zeros_(m.bias)
                    ecg_projector.apply(_init_weights)
                    logger.info(f"Initialized ECG projector weights on {device} (was meta)")
                else:
                    ecg_projector = ecg_projector.to(device)
                
                # æŒ‚è½½
                self.model.ecg_tower = ecg_tower
                self.model.ecg_projector = ecg_projector
                
                # ä¿å­˜ Config
                config.ecg_tower_path = ecg_tower_path
                config.ecg_projector_type = ecg_projector_type
                config.ecg_model_config = ecg_model_config
                config.ecg_hidden_size = ecg_hidden_size
                
                logger.info('âœ… ECG components attached successfully.')
            except Exception as e:
                logger.error(f'âŒ Failed to initialize ECG components: {e}')
                raise e

    def forward(self, ecg_features: Optional[torch.FloatTensor] = None, **kwargs):
        """
        å¤–å±‚ forwardã€‚æ˜¾å¼æ¥æ”¶ ecg_features å¹¶é€ä¼ ã€‚
        """
        return super().forward(ecg_features=ecg_features, **kwargs)


# ==================== 5. ECGR1Template å®šä¹‰ ====================

class ECGR1Template(Qwen3VLTemplate):
    """
    ECG-R1 æ¨¡æ¿ï¼Œç»§æ‰¿è‡ª Qwen3VLTemplateã€‚
    """
    version = 'v3'
    ecg_placeholder = '<|ecg_pad|>'
    ecg_start_token = '<|ecg_start|>'
    ecg_end_token = '<|ecg_end|>'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ecg_seq_length = get_env_args('ECG_SEQ_LENGTH', int, 5000)
        self.ecg_patch_size = get_env_args('ECG_PATCH_SIZE', int, 50)
        self.ecg_num_patches = self.ecg_seq_length // self.ecg_patch_size
        self.root_ecg_dir = get_env_args('ROOT_ECG_DIR', str, None)
        self.interleave_prob = get_env_args('INTERLEAVE_PROB', float, 0.1)
        self.modality_dropout_prob = get_env_args('MODALITY_DROPOUT_PROB', float, 0.5)
        try:
            seed = torch.initial_seed()
        except Exception:
            seed = 42
        self._rng = random.Random(seed)
        # ECG Token ID å°†åœ¨ init_processor ä¸­è®¾ç½®ï¼Œå› ä¸ºæ­¤æ—¶ processor å¯èƒ½è¿˜æœªåˆå§‹åŒ–

    def init_processor(self, processor) -> None:
        """é‡å†™ init_processorï¼Œåœ¨ processor è®¾ç½®åæ³¨å†Œ ECG Token ID"""
        super().init_processor(processor)
        # ç°åœ¨ processor å·²ç»è®¾ç½®ï¼Œå¯ä»¥å®‰å…¨åœ°è®¿é—®å®ƒ
        if hasattr(self, 'processor') and self.processor is not None:
            tokenizer = self.processor.tokenizer if hasattr(self.processor, 'tokenizer') else self.processor
            self.ecg_token_id = tokenizer.convert_tokens_to_ids(self.ecg_placeholder)
            self.ecg_start_token_id = tokenizer.convert_tokens_to_ids(self.ecg_start_token)
            self.ecg_end_token_id = tokenizer.convert_tokens_to_ids(self.ecg_end_token)
            
            if self.ecg_token_id not in self.placeholder_tokens:
                self.placeholder_tokens.append(self.ecg_token_id)

    def replace_ecg(self, ecg_data: Any, index: int, inputs: StdTemplateInputs) -> List[Context]:
        """åŠ è½½æ•°æ®å¹¶è¿”å›å ä½ç¬¦"""
        ecgs = inputs.objects.get('ecg', [])
        if index < len(ecgs):
            ecg = ecgs[index]
            if isinstance(ecg, str):
                ecgs[index] = load_ecg(ecg, self.ecg_seq_length, self.root_ecg_dir)
        return [self.ecg_start_token, self.ecg_placeholder, self.ecg_end_token]

    # ===== Interleave & Modality Dropout helpers =====
    def _remove_ecg_tag(self, text: str) -> str:
        text = re.sub(r'\s*<ecg>\s*', ' ', text)
        text = re.sub(r'\s{2,}', ' ', text)
        return text.strip()

    def _remove_image_tag(self, text: str) -> str:
        text = re.sub(r'\s*<image>\s*', ' ', text)
        text = re.sub(r'\s{2,}', ' ', text)
        return text.strip()

    def _swap_ecg_image(self, text: str) -> str:
        # Try ecg->image, if none swapped try image->ecg
        new_text, n = re.subn(r'<ecg>(\s*)<image>', r'<image>\1<ecg>', text)
        if n == 0:
            new_text, _ = re.subn(r'<image>(\s*)<ecg>', r'<ecg>\1<image>', text)
        return new_text

    def _restore_one_modality(self, inputs: StdTemplateInputs, orig_messages, orig_ecg, orig_images, prefer: str = 'image'):
        if prefer == 'image' and orig_images:
            inputs.images = copy.deepcopy(orig_images)
        if prefer == 'ecg' and orig_ecg:
            inputs.objects['ecg'] = copy.deepcopy(orig_ecg)
        # å¦‚æœä»ä¸ºç©ºï¼Œå›é€€åˆ°åŸå§‹
        if not inputs.images and orig_images:
            inputs.images = copy.deepcopy(orig_images)
        if (not inputs.objects.get('ecg')) and orig_ecg:
            inputs.objects['ecg'] = copy.deepcopy(orig_ecg)
        inputs.messages = copy.deepcopy(orig_messages)
        return inputs

    def _maybe_interleave_and_dropout(self, inputs: StdTemplateInputs) -> StdTemplateInputs:
        if self.mode != 'train':
            return inputs

        # å¤‡ä»½åŸå§‹å†…å®¹ä»¥ä¾¿å›é€€
        orig_messages = copy.deepcopy(inputs.messages)
        orig_ecg = copy.deepcopy(inputs.objects.get('ecg', []))
        orig_images = copy.deepcopy(getattr(inputs, 'images', []))

        rng = self._rng
        has_ecg = bool(orig_ecg)
        has_img = bool(orig_images)


        # å•æ¦‚ç‡æ¨¡æ€ä¸¢å¼ƒï¼šåœ¨å¯ç”¨æ¨¡æ€ä¸­éšæœºé€‰ä¸€ä¾§
        if rng.random() < self.modality_dropout_prob and (has_ecg or has_img):
            candidates = []
            if has_ecg:
                candidates.append('ecg')
            if has_img:
                candidates.append('image')
            if candidates:
                choice = rng.choice(candidates)
                if choice == 'ecg':
                    inputs.objects['ecg'] = []
                    inputs.messages = [
                        {**m, 'content': self._remove_ecg_tag(m['content'])} if m.get('role') == 'user' else m
                        for m in inputs.messages
                    ]
                elif choice == 'image':
                    inputs.images = []
                    inputs.messages = [
                        {**m, 'content': self._remove_image_tag(m['content'])} if m.get('role') == 'user' else m
                        for m in inputs.messages
                    ]

        # é¡ºåºéšæœºåŒ–
        if rng.random() < self.interleave_prob:
            inputs.messages = [
                {**m, 'content': self._swap_ecg_image(m['content'])} if m.get('role') == 'user' else m
                for m in inputs.messages
            ]

        # å®ˆæŠ¤ï¼šé¿å…ä¸¤æ¨¡æ€éƒ½è¢«å»é™¤
        if not inputs.objects.get('ecg') and not getattr(inputs, 'images', []):
            inputs = self._restore_one_modality(inputs, orig_messages, orig_ecg, orig_images, prefer='image')

        return inputs

    def _pre_tokenize(self, context_list: List[Context], loss_scale_list: List[float], inputs: StdTemplateInputs):
        """
        1. å¤„ç† <ecg> æ ‡ç­¾ï¼šæ‹†åˆ†å¹¶æ›¿æ¢ä¸º tokenã€‚
        2. è°ƒç”¨ super()._pre_tokenize()ï¼Œç”±çˆ¶ç±» Qwen3VLTemplate å¤„ç†å‰©ä½™çš„ image/videoã€‚
        """
        new_ctx, new_loss = [], []
        inputs.ecg_idx = 0 # ç¡®ä¿ç´¢å¼•ä»0å¼€å§‹

        for ctx, loss in zip(context_list, loss_scale_list):
            if isinstance(ctx, str) and '<ecg>' in ctx:
                parts = re.split(r'(<ecg>)', ctx)
                for part in parts:
                    if part == '<ecg>':
                        # æ›¿æ¢ä¸º ECG Tokensï¼Œloss ç½®ä¸º 0
                        c_list = self.replace_ecg(None, inputs.ecg_idx, inputs)
                        inputs.ecg_idx += 1
                        new_ctx.extend(c_list)
                        new_loss.extend([0.0] * len(c_list))
                    elif part: # éç©ºå­—ç¬¦ä¸²
                        new_ctx.append(part)
                        new_loss.append(loss)
            else:
                new_ctx.append(ctx)
                new_loss.append(loss)
        
        # å°†å¤„ç†å®Œ ECG çš„åˆ—è¡¨ä¼ ç»™çˆ¶ç±»ï¼Œçˆ¶ç±»ä¼šå¤„ç†å‰©ä¸‹çš„ <image>/<video>
        return super()._pre_tokenize(new_ctx, new_loss, inputs)

    def _encode(self, inputs: StdTemplateInputs) -> Dict[str, Any]:
        """
        1. è°ƒç”¨ super()._encode() ç”Ÿæˆ input_ids å’Œ visual tensorã€‚
        2. è¡¥å…… ECG ç‰¹æœ‰çš„ Tensor å †å å’Œ ID æ‰©å±•é€»è¾‘ã€‚
        3. è®¾ç½® mm_processor_kwargs ä»¥ä¾¿ vLLM ä½¿ç”¨æ­£ç¡®çš„å›¾åƒå‚æ•°ã€‚
        """
        inputs = self._maybe_interleave_and_dropout(inputs)
        encoded = super()._encode(inputs)
        
        # è°ƒç”¨ ECG åå¤„ç†
        return self._postprocess_ecg(encoded, inputs)
    
    def _encode_truncated(self, inputs: StdTemplateInputs) -> Dict[str, Any]:
        """
        é‡å†™ _encode_truncated ä»¥ç¡®ä¿åœ¨ vLLM æ¨¡å¼ä¸‹ä¹Ÿå¤„ç† ECG æ•°æ®ã€‚
        
        âš ï¸ å…³é”®ï¼šçˆ¶ç±»åœ¨ vLLM æ¨¡å¼ä¸‹ä¼šè·³è¿‡å­ç±»çš„ _encode()ï¼Œç›´æ¥è°ƒç”¨ Template._encode()ã€‚
        æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œæ·»åŠ  ECG åå¤„ç†é€»è¾‘ã€‚
        """
        # è°ƒç”¨çˆ¶ç±»çš„ _encode_truncated
        inputs = self._maybe_interleave_and_dropout(inputs)
        encoded = super()._encode_truncated(inputs)
        
        # å¦‚æœæ˜¯ vLLM æ¨¡å¼ï¼Œçˆ¶ç±»è·³è¿‡äº†æˆ‘ä»¬çš„ _encodeï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç† ECG
        if self.mode in {'vllm', 'lmdeploy', 'sglang'}:
            encoded = self._postprocess_ecg(encoded, inputs)
        
        return encoded
    
    def _postprocess_ecg(self, encoded: Dict[str, Any], inputs: StdTemplateInputs) -> Dict[str, Any]:
        """
        ECG åå¤„ç†ï¼šæ‰©å±• token å’ŒåŠ è½½æ•°æ®ã€‚
        æŠ½å–ä¸ºç‹¬ç«‹æ–¹æ³•ï¼Œä¾› _encode å’Œ _encode_truncated è°ƒç”¨ã€‚
        """
        # è®¾ç½® mm_processor_kwargs (è®© vLLM ä½¿ç”¨æ­£ç¡®çš„å›¾åƒå‚æ•°)
        # vLLM ä¸ä¼šè°ƒç”¨ patch_qwen_vl_utilsï¼Œéœ€è¦æ˜¾å¼ä¼ é€’è¿™äº›å‚æ•°
        # æ³¨æ„ï¼šéœ€è¦åŒæ—¶è®¾ç½® inputs å’Œ encodedï¼Œå› ä¸ºåœ¨ vLLM æ¨¡å¼ä¸‹çˆ¶ç±»å¯èƒ½å·²ç»å¤„ç†è¿‡
        factor = 32  # patch_size(16) Ã— merge_size(2) for Qwen3VL
        max_tokens = int(os.environ.get('IMAGE_MAX_TOKEN_NUM', '768'))
        min_tokens = int(os.environ.get('IMAGE_MIN_TOKEN_NUM', '4'))
        mm_processor_kwargs = {
            'min_pixels': min_tokens * (factor ** 2),  # 4 Ã— 32Â² = 4,096
            'max_pixels': max_tokens * (factor ** 2),  # 768 Ã— 32Â² = 786,432
        }
        inputs.mm_processor_kwargs = mm_processor_kwargs
        encoded['mm_processor_kwargs'] = mm_processor_kwargs  # ç¡®ä¿ vLLM æ¨¡å¼ä¸‹ä¹Ÿç”Ÿæ•ˆ
        
        input_ids = encoded['input_ids']
        
        ecgs = inputs.objects.get('ecg', [])
        if ecgs:
            # æ‰©å±• Token ID (1ä¸ª placeholder -> N+1 ä¸ªçœŸå® token)
            idx_list = findall(input_ids, self.ecg_token_id)
            if idx_list:
                tokens_per_ecg = self.ecg_num_patches + 1 # +1 æ˜¯ cls token
                def _get_tokens(i): return [self.ecg_token_id] * tokens_per_ecg
                
                input_ids, encoded['labels'], encoded['loss_scale'] = self._extend_tokens(
                    input_ids, encoded['labels'], encoded.get('loss_scale'), idx_list, _get_tokens
                )
            
            # å †å  Tensor
            tensor_list = []
            for item in ecgs:
                if isinstance(item, str): 
                    item = load_ecg(item, self.ecg_seq_length, self.root_ecg_dir)
                tensor_list.append(item)
            
            if tensor_list:
                encoded['ecg_features'] = torch.stack(tensor_list)
        
        encoded['input_ids'] = input_ids
        return encoded
    
    def _data_collator_mm_data(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Collate æ—¶æ‹¼æ¥ ECG Features"""
        res = super()._data_collator_mm_data(batch)
        ecg_features = [b['ecg_features'] for b in batch if b.get('ecg_features') is not None]
        if ecg_features:
            res['ecg_features'] = torch.cat(ecg_features, dim=0)
        return res
    
    def normalize_bbox(self, inputs: StdTemplateInputs):
        """
        [Fix] é‡å†™ä»¥é˜²æ­¢ KeyError: 'bbox'
        çˆ¶ç±»é€»è¾‘å‡è®¾ inputs.objects éç©ºå°±ä¸€å®šåŒ…å« bboxï¼Œ
        ä½†æˆ‘ä»¬è¿™é‡Œå¯èƒ½åªæœ‰ ecg æ•°æ®ã€‚
        """
        if inputs.objects and 'bbox' in inputs.objects:
            return super().normalize_bbox(inputs)
        
        # å¦‚æœæ²¡æœ‰ bbox (ä¾‹å¦‚åªæœ‰ ecg)ï¼Œä»€ä¹ˆéƒ½ä¸åšï¼Œç›´æ¥è¿”å›
        return
    
register_template(
    QwenTemplateMeta(
        'ecg_r1',
        template_cls=ECGR1Template,
        default_system='You are a helpful assistant.',
    ))


# ==================== 6. æ³¨å†Œä¸æµ‹è¯•å…¥å£ ====================

register_model_arch(
    MultiModelKeys(
        'ecg_r1',
        language_model='model.language_model',
        vision_tower=['model.visual', 'model.ecg_tower'],
        aligner=['model.visual.merger', 'model.visual.deepstack_merger_list', 'model.ecg_projector'],
    )
)

def get_model_tokenizer_ecg_r1(model_dir, model_info, model_kwargs, load_model=True, **kwargs):
    kwargs['automodel_class'] = ECGR1ForConditionalGeneration
    kwargs['_check_qwen_vl_utils'] = False 
    
    from swift.llm.model.model.qwen import get_model_tokenizer_qwen2_vl
    model, processor = get_model_tokenizer_qwen2_vl(model_dir, model_info, model_kwargs, load_model, **kwargs)
    
    # æ·»åŠ  Special Tokens
    if processor is not None:
        tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else processor
        ecg_tokens = ['<|ecg_pad|>', '<|ecg_start|>', '<|ecg_end|>']
        tokens_to_add = [t for t in ecg_tokens if t not in tokenizer.get_vocab()]
        
        if tokens_to_add:
            num = tokenizer.add_special_tokens({'additional_special_tokens': tokens_to_add})
            if model is not None and num > 0:
                model.resize_token_embeddings(len(tokenizer))
                model.config.ecg_token_id = tokenizer.convert_tokens_to_ids('<|ecg_pad|>')

    # è®¾ç½®å†»ç»“çŠ¶æ€
    if model and load_model and hasattr(model.model, 'ecg_tower'):
        # 1. è·å–ç¯å¢ƒå˜é‡æ§åˆ¶ (é»˜è®¤éƒ½è®­ç»ƒ)
        freeze_tower = get_env_args('FREEZE_ECG_TOWER', bool, False)
        freeze_projector = get_env_args('FREEZE_ECG_PROJECTOR', bool, False)
        
        # 2. å®šä¹‰ç»Ÿä¸€çš„å¤„ç†å‡½æ•°
        def _set_module_state(module, is_frozen, name):
            if is_frozen:
                module.eval()
                for param in module.parameters():
                    param.requires_grad = False
                logger.info(f"{name}: Frozen (eval mode, requires_grad=False)")
            else:
                module.train()
                for param in module.parameters():
                    param.requires_grad = True
                logger.info(f"{name}: Trainable (train mode, requires_grad=True)")

        # 3. åº”ç”¨çŠ¶æ€
        _set_module_state(model.model.ecg_tower, freeze_tower, "ECG Tower")
        _set_module_state(model.model.ecg_projector, freeze_projector, "ECG Projector")

    return model, processor

register_model(ModelMeta(
    'ecg_r1', [], 'ecg_r1', get_model_tokenizer_ecg_r1, 
    is_multimodal=True, model_arch='ecg_r1', 
    architectures=['Qwen3VLForConditionalGeneration', 'ECGR1ForConditionalGeneration'], 
    tags=['vision', 'ecg']
))

if __name__ == '__main__':
    # é™åˆ¶åªä½¿ç”¨ç¬¬ä¸€å¼  GPUï¼Œé¿å…å¤šå¡è®¾å¤‡ä¸ä¸€è‡´é—®é¢˜
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    os.environ['ECG_SEQ_LENGTH'] = '5000'
    os.environ['ECG_PATCH_SIZE'] = '50'
    os.environ['ROOT_ECG_DIR'] = '/data/jinjiarui/datasets/ECG_R1_Dataset/ecg_timeseries'
    os.environ['ROOT_IMAGE_DIR'] = "/data/jinjiarui/datasets/ECG_R1_Dataset/ecg_images"
    os.environ['IMAGE_MAX_TOKEN_NUM'] = '768'
    os.environ['ECG_TOWER_PATH'] = 'ecg_coca/checkpoint/cpt_wfep_epoch_20.pt'
    os.environ['ECG_PROJECTOR_TYPE'] = 'mlp2x_gelu'
    os.environ['ECG_MODEL_CONFIG'] = 'coca_ViT-B-32'
    # å…³é—­ interleave/dropoutï¼Œé¿å…æµ‹è¯•æ—¶ç¼ºæ¨¡æ€
    os.environ['INTERLEAVE_PROB'] = '0'
    os.environ['MODALITY_DROPOUT_PROB'] = '0'
    
    # è®¾ç½® ECG è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨ï¼‰
    os.environ['FREEZE_ECG_TOWER'] = 'True'  # è®­ç»ƒ ECG tower
    os.environ['FREEZE_ECG_PROJECTOR'] = 'False'  # è®­ç»ƒ ECG projector

    # æµ‹è¯•ä¸debug
    model, processor = get_model_tokenizer('Qwen/Qwen3-VL-8B-Instruct', model_type='ecg_r1')
    

    # æ£€æŸ¥ ECG ç»„ä»¶æ˜¯å¦åŠ è½½
    has_ecg_tower = hasattr(model.model, 'ecg_tower')
    has_ecg_projector = hasattr(model.model, 'ecg_projector')
    print(f'\nğŸ” ECG Components Status:')
    print(f'   ECG Tower loaded: {has_ecg_tower}')
    print(f'   ECG Projector loaded: {has_ecg_projector}')
    if not has_ecg_tower:
        print(f'   âš ï¸  Tip: Set ECG_TOWER_PATH environment variable to load ECG tower')
    print()
    
    template = get_template('ecg_r1', processor)
    # ç¡®ä¿æµ‹è¯•æ—¶ä¸åšéšæœºä¸¢å¼ƒ/äº’æ¢
    template.interleave_prob = 0.0
    template.modality_dropout_prob = 0.0
    template._rng = random.Random(42)
    data = {
        'messages': [
            {'role': 'user', 'content': '<ecg><image>\nTime for a multiple-choice challenge! Share your thought process, then lock in your final answer.\nWhat can be inferred about the cardiac axis on this ECG?\nA. The axis is normal\nB. The axis is deviated to the right\nC. The axis is indeterminate\nD. The axis is deviated to the left'},
            {'role': 'assistant', 'content': 'The ECG shows no clear indication of a specific axis deviation, and the QRS morphology does not suggest a clear right or left axis deviation. This image indicates that the cardiac axis is indeterminate, meaning that it cannot be determined based on the ECG findings.\n\nTherefore, we choose C. The axis is indeterminate'},
        ],
        'images': ['mimic/p1127/p11273115/s44111511/44111511-0.png'],
        'objects': {'ecg': ['mimic-iv/files/p1127/p11273115/s44111511/44111511']},
    }
    template.set_mode('train')
    encoded = template.encode(data)
    
    # æ£€æŸ¥ ECG token
    ecg_token_ids = template._tokenize(template.ecg_placeholder)
    image_token_id = template.image_token_id
    print(f'\n=== Token Info ===')
    print(f'ECG placeholder: {template.ecg_placeholder}')
    print(f'ECG token IDs: {ecg_token_ids}')
    print(f'Image token ID: {image_token_id}')
    print(f'placeholder_tokens: {template.placeholder_tokens}')
    
    # ç»Ÿè®¡ input_ids ä¸­çš„ token
    input_ids_list = encoded['input_ids']
    labels_list = encoded['labels']
    if isinstance(ecg_token_ids, list) and len(ecg_token_ids) > 0:
        ecg_token_id = ecg_token_ids[0]
        ecg_count = input_ids_list.count(ecg_token_id)
        print(f'ECG token {ecg_token_id} appears {ecg_count} times in input_ids')
    image_count = input_ids_list.count(image_token_id)
    print(f'Image token {image_token_id} appears {image_count} times in input_ids')
    
    print('\n=== Decoded ===')
    print('input_ids: ' + template.safe_decode(encoded['input_ids']))
    print('labels: ' + template.safe_decode(encoded['labels']))
    print('keys: ' + str(encoded.keys()))
    
    # æ‰“å°è¯¦ç»†ä¿¡æ¯
    print(f'\n=== Detailed Info ===')
    print(f'input_ids length: {len(encoded["input_ids"])}')
    print(f'labels length: {len(encoded["labels"])}')
    if 'ecg_features' in encoded:
        print(f'ecg_features shape: {encoded["ecg_features"].shape}')
    if 'pixel_values' in encoded:
        print(f'pixel_values shape: {encoded["pixel_values"].shape}')
    if 'image_grid_thw' in encoded:
        print(f'image_grid_thw: {encoded["image_grid_thw"]}')
    
    # ========== è¯¦ç»†çš„ Label Mask éªŒè¯ ==========
    print(f'\n{"="*80}')
    print('=== Label Mask Validation ===')
    print(f'{"="*80}')
    
    # 1. ç»Ÿè®¡ labels ä¸­çš„ -100ï¼ˆä¸è®¡ç®—lossçš„ä½ç½®ï¼‰
    num_ignore = sum(1 for label in labels_list if label == -100)
    num_train = len(labels_list) - num_ignore
    print(f'\n1. Label Statistics:')
    print(f'   Total tokens: {len(labels_list)}')
    print(f'   Ignored tokens (label=-100): {num_ignore} ({num_ignore/len(labels_list)*100:.1f}%)')
    print(f'   Training tokens (label!=-100): {num_train} ({num_train/len(labels_list)*100:.1f}%)')
    
    # 2. éªŒè¯ç‰¹æ®Š token çš„ label æ˜¯å¦ä¸º -100
    print(f'\n2. Special Token Label Check:')
    special_tokens = {
        'ECG pad': template.ecg_token_id,
        'ECG start': template.ecg_start_token_id,
        'ECG end': template.ecg_end_token_id,
        'Image': template.image_token_id,
        'Vision start': template.processor.tokenizer.convert_tokens_to_ids('<|vision_start|>'),
        'Vision end': template.processor.tokenizer.convert_tokens_to_ids('<|vision_end|>'),
    }
    
    for name, token_id in special_tokens.items():
        positions = [i for i, tok in enumerate(input_ids_list) if tok == token_id]
        if positions:
            label_values = [labels_list[i] for i in positions[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
            all_masked = all(labels_list[i] == -100 for i in positions)
            status = 'âœ“ All masked' if all_masked else 'âœ— Some not masked'
            print(f'   {name:15} (ID={token_id:6}): {len(positions):4} occurrences, {status}')
            if len(positions) <= 5:
                print(f'      Labels at positions {positions}: {label_values}')
    
    # 3. æ‰¾åˆ° assistant çš„å›å¤éƒ¨åˆ†
    print(f'\n3. Assistant Response Check:')
    tokenizer = template.processor.tokenizer
    im_start_id = tokenizer.convert_tokens_to_ids('<|im_start|>')
    im_end_id = tokenizer.convert_tokens_to_ids('<|im_end|>')
    
    # æ‰¾åˆ°æœ€åä¸€ä¸ª <|im_start|>assistant
    assistant_token = tokenizer.encode('assistant', add_special_tokens=False)[0]
    assistant_start = None
    for i in range(len(input_ids_list) - 1):
        if input_ids_list[i] == im_start_id and input_ids_list[i+1] == assistant_token:
            assistant_start = i
    
    if assistant_start is not None:
        # æ‰¾åˆ°å¯¹åº”çš„ <|im_end|>
        assistant_end = None
        for i in range(assistant_start + 1, len(input_ids_list)):
            if input_ids_list[i] == im_end_id:
                assistant_end = i
                break
        
        if assistant_end is not None:
            assistant_content = input_ids_list[assistant_start:assistant_end+1]
            assistant_labels = labels_list[assistant_start:assistant_end+1]
            
            # ç»Ÿè®¡ assistant éƒ¨åˆ†çš„ label
            assistant_ignore = sum(1 for label in assistant_labels if label == -100)
            assistant_train = len(assistant_labels) - assistant_ignore
            
            print(f'   Assistant tokens range: [{assistant_start}, {assistant_end}] (length={len(assistant_content)})')
            print(f'   Assistant ignored tokens: {assistant_ignore}')
            print(f'   Assistant training tokens: {assistant_train}')
            
            # æ˜¾ç¤ºå‰å‡ ä¸ª token çš„ label
            print(f'\n   First 10 tokens in assistant response:')
            for i in range(min(10, len(assistant_content))):
                idx = assistant_start + i
                token_str = tokenizer.decode([input_ids_list[idx]])
                label_str = 'IGNORE' if labels_list[idx] == -100 else str(labels_list[idx])
                print(f'      [{idx:4}] Token: {token_str:20} | Label: {label_str}')
    
    # 4. éªŒè¯ input_ids å’Œ labels çš„å¯¹é½
    print(f'\n4. Input-Label Alignment Check:')
    misaligned = []
    for i in range(len(input_ids_list)):
        if labels_list[i] != -100 and labels_list[i] != input_ids_list[i]:
            misaligned.append((i, input_ids_list[i], labels_list[i]))
    
    if misaligned:
        print(f'   âœ— Found {len(misaligned)} misaligned positions:')
        for pos, input_id, label in misaligned[:5]:
            print(f'      Position {pos}: input_id={input_id}, label={label}')
    else:
        print(f'   âœ“ All labels are either -100 or equal to input_ids (correct!)')
    
    # 5. æ£€æŸ¥ loss_scaleï¼ˆå¦‚æœæœ‰ï¼‰
    if 'loss_scale' in encoded and encoded['loss_scale'] is not None:
        print(f'\n5. Loss Scale Check:')
        loss_scale = encoded['loss_scale']
        print(f'   Loss scale length: {len(loss_scale)}')
        unique_scales = set(loss_scale)
        print(f'   Unique loss scale values: {sorted(unique_scales)}')
        for scale in sorted(unique_scales):
            count = sum(1 for s in loss_scale if s == scale)
            print(f'      Scale {scale}: {count} tokens ({count/len(loss_scale)*100:.1f}%)')
    
    print(f'\n{"="*80}')
    print('=== Validation Complete ===')
    print(f'{"="*80}\n')
    
    # ========== æµ‹è¯• Forward Pass ==========
    if hasattr(model.model, 'ecg_tower'):
        print(f'{"="*80}')
        print('=== Forward Pass Test ===')
        print(f'{"="*80}')
        
        try:
            import torch
            
            # --- ä¿®å¤ 1ï¼šåŠ¨æ€è·å–å®é™…è®¾å¤‡ ---
            # æ‰¾åˆ° ECG Tower æ‰€åœ¨çš„å®é™…è®¾å¤‡ (å³æƒé‡æ‰€åœ¨çš„è®¾å¤‡ï¼Œè¿™é€šå¸¸æ˜¯ Accelerate æ”¾ç½®çš„è®¾å¤‡)
            device = next(model.model.ecg_tower.parameters()).device
            print(f"ğŸ¯ Target Device determined from ECG Tower: {device}")
            
            # ç¡®ä¿ model çš„æ‰€æœ‰å­æ¨¡å—éƒ½ä½äºè¯¥è®¾å¤‡ï¼ˆè™½ç„¶ Accelerate ä¼šå¤„ç†ï¼Œä½†æ‰‹åŠ¨ç»Ÿä¸€æ›´ä¿é™©ï¼‰
            if str(device).startswith('cuda'):
                model.to(device)
            
            # --- ä¿®å¤ 2ï¼šç»Ÿä¸€è¾“å…¥æ•°æ®çš„è®¾å¤‡ ---
            inputs = {
                'input_ids': torch.tensor([encoded['input_ids']]).to(device),
                'labels': torch.tensor([encoded['labels']]).to(device),
            }
            if 'pixel_values' in encoded:
                inputs['pixel_values'] = encoded['pixel_values'].unsqueeze(0).to(device)
            if 'image_grid_thw' in encoded:
                inputs['image_grid_thw'] = encoded['image_grid_thw'].to(device)
            if 'ecg_features' in encoded:
                inputs['ecg_features'] = encoded['ecg_features'].to(device)
            
            # ç¡®ä¿ attention mask å­˜åœ¨ä¸”åœ¨ç›®æ ‡è®¾å¤‡ä¸Š
            seq_len = inputs['input_ids'].shape[1]
            inputs['attention_mask'] = torch.ones(1, seq_len).to(device)
            
            print(f'\n1. Input Shapes:')
            for key, val in inputs.items():
                if isinstance(val, torch.Tensor):
                    print(f'   {key:20}: {list(val.shape)}')
            
            # Forward pass
            print(f'\n2. Running forward pass...')
            model.eval()
            with torch.no_grad():
                outputs = model(**inputs)
            
            print(f'   âœ“ Forward pass successful!')
            
            # æ£€æŸ¥è¾“å‡º
            print(f'\n3. Output Information:')
            print(f'   Loss: {outputs.loss.item():.4f}')
            print(f'   Logits shape: {list(outputs.logits.shape)}')
            
            # éªŒè¯ ECG embeddings
            # æ³¨æ„ï¼šç”±äºåœ¨ forward ä¸­æ‰è¿›è¡Œèåˆï¼Œè¿™é‡Œæ— æ³•ç›´æ¥çœ‹åˆ° embedding èåˆåçš„ç»“æœ
            # ä½†æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ logits
            print(f'\n4. ECG Processing Verification:')
            
            # è®¡ç®—è®­ç»ƒ token çš„å¹³å‡ loss
            trainable_positions = [i for i, label in enumerate(encoded['labels']) if label != -100]
            if trainable_positions:
                print(f'\n5. Training Token Analysis:')
                print(f'   Trainable positions: {len(trainable_positions)}')
                
                # è®¡ç®— per-token loss
                shift_logits = outputs.logits[0, :-1, :]
                shift_labels = inputs['labels'][0, 1:]
                
                # åªè®¡ç®—é -100 çš„ token
                mask = shift_labels != -100
                if mask.sum() > 0:
                    from torch.nn import functional as F
                    token_losses = F.cross_entropy(
                        shift_logits[mask], 
                        shift_labels[mask], 
                        reduction='none'
                    )
                    print(f'   Per-token loss stats:')
                    print(f'      Mean: {token_losses.mean().item():.4f}')
                    print(f'      Min:  {token_losses.min().item():.4f}')
                    print(f'      Max:  {token_losses.max().item():.4f}')
            
            print(f'\n   âœ… All forward pass tests passed!')
            
        except Exception as e:
            print(f'\n   âŒ Forward pass failed with error:')
            print(f'   Error type: {type(e).__name__}')
            print(f'   Error message: {str(e)}')
            import traceback
            print(f'\n   Traceback:')
            traceback.print_exc()
        
        print(f'\n{"="*80}\n')
    else:
        print(f'\nâš ï¸  Skipping forward pass test (ECG tower not loaded)\n')

    # ========== å•å…ƒæµ‹è¯•ï¼šInterleave & Dropout è¾…åŠ©å‡½æ•° ==========
    print(f'{"="*80}')
    print('=== Interleave & Dropout Helper Tests ===')
    print(f'{"="*80}')

    # åŸºç¡€æ ·æœ¬ï¼šç®€å•å ä½ç¬¦ç¤ºä¾‹ï¼Œé¿å…å¹²æ‰°ä¸»æ•°æ®
    sample_inputs = StdTemplateInputs(
        messages=[{'role': 'user', 'content': '<ecg> <image>'}],
        images=['img'],
        objects={'ecg': ['ecg']}
    )

    # 1) äº’æ¢å ä½ç¬¦
    swapped = template._swap_ecg_image(sample_inputs.messages[0]['content'])
    print(f'swap_ecg_image: "{sample_inputs.messages[0]["content"]}" -> "{swapped}"')

    # 2) åˆ é™¤ ECG tag
    removed_ecg = template._remove_ecg_tag('<ecg> hello <image>')
    print(f'remove_ecg_tag: "<ecg> hello <image>" -> "{removed_ecg}"')

    # 3) åˆ é™¤ image tag
    removed_img = template._remove_image_tag('<ecg> hello <image>')
    print(f'remove_image_tag: "<ecg> hello <image>" -> "{removed_img}"')

    # 4) æ¨¡æ€ä¸¢å¼ƒå®ˆæŠ¤ï¼šåŒæ—¶æ¸…ç©ºåº”æ¢å¤ä¸€ä¾§
    tmp_inputs = StdTemplateInputs(
        messages=[{'role': 'user', 'content': '<ecg> <image>'}],
        objects={'ecg': ['ecg']},
        images=['img']
    )
    tmp_inputs.objects['ecg'] = []
    tmp_inputs.images = []
    restored = template._restore_one_modality(
        tmp_inputs,
        orig_messages=[{'role': 'user', 'content': '<ecg> <image>'}],
        orig_ecg=['ecg'],
        orig_images=['img'],
        prefer='image'
    )
    assert restored.images or restored.objects.get('ecg'), "restore_one_modality failed: both empty"
    print('âœ“ restore_one_modality ok')


# ==================== è®­ç»ƒçŠ¶æ€ç›‘æ§å›è°ƒ ====================
# å¯¼å…¥è®­ç»ƒçŠ¶æ€æ‰“å°å›è°ƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    import importlib.util
    import os
    callback_path = os.path.join(os.path.dirname(__file__), 'training_status_callback.py')
    spec = importlib.util.spec_from_file_location("training_status_callback", callback_path)
    callback_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(callback_module)
    logger.info('âœ… Training status callback loaded and registered.')
except Exception as e:
    logger.warning(f'âš ï¸ Failed to load training status callback: {e}')



from typing import Any, Dict, Optional

from swift.llm import DatasetMeta, MessagesPreprocessor, load_dataset, register_dataset


class ECGR1Preprocessor(MessagesPreprocessor):
    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        from copy import deepcopy
        messages = deepcopy(row.get('messages', []))
        
        # ç§»é™¤æ•°æ®é›†ä¸­çš„ system messageï¼Œè®© --system å‚æ•°ç”Ÿæ•ˆ
        if messages and messages[0].get('role') == 'system':
            messages.pop(0)
        

        row['messages'] = messages
        return super().preprocess(row)

register_dataset(
    DatasetMeta(
        dataset_path='/data/jinjiarui/datasets/ECG_R1_Dataset/ecg_jsons/ECG_R1_Structured_CoT/wo_protocol/ECG_R1_Structured_CoT_RL_dataset_2k_with_solution_full.jsonl',
        dataset_name='ecg_r1_structured_cot_rl_dataset_2k',
        preprocess_func=ECGR1Preprocessor(),
        tags=['ecg', 'grpo', 'vision']))

# register_dataset(
#     DatasetMeta(
#         dataset_path='/data/jinjiarui/datasets/ECG_R1_Dataset/ecg_jsons/ECG_R1_Structured_CoT/w_protocol/ECG_R1_Structured_CoT_RL_dataset_2k_with_solution_full_with_protocol.jsonl',
#         dataset_name='ecg_r1_structured_cot_rl_dataset_2k_with_protocol',
#         preprocess_func=ECGR1Preprocessor(),
#         tags=['ecg', 'grpo', 'vision']))